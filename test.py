import torch
import numpy as np
import torch.nn.functional as F
# from buff import focal_loss
# from seq_encoder import FofeSeqEncoder, RNNSeqEncoder
import re
import pdb
from buff import time_record
from attention import *

# from transformer.sub_layers import ScaledDotProductAttention, MultiHeadAttention

a = 213
abc = torch.tensor([1, 2, 4])
import ipdb

ipdb.set_trace()

print('f')
